import pytest
import json
from unittest.mock import AsyncMock, patch
from backend.app.services.nlg.nlg_engine import NLGEngine
from backend.app.services.nlg.llm_client import LLMClient
from backend.app.services.nlg.prompt_templates import get_template, fill_template

# Concrete implementation for testing purposes
class ConcreteNLGEngine(NLGEngine):
    async def generate_section_text(self, section_id: str, raw_data: dict) -> str:
        # This is a placeholder implementation for the abstract method
        return json.dumps({"section_id": section_id, "text": "Generated by ConcreteNLGEngine"})

# Mock the LLMClient for all tests in this module
@pytest.fixture
def mock_llm_client():
    with patch('backend.app.services.nlg.nlg_engine.LLMClient', autospec=True) as MockLLMClient:
        mock_instance = MockLLMClient.return_value
        mock_instance.__aenter__.return_value = mock_instance
        mock_instance.__aexit__.return_value = None
        mock_instance.generate_text = AsyncMock()
        yield mock_instance

@pytest.fixture
def nlg_engine():
    return ConcreteNLGEngine()

@pytest.mark.asyncio
async def test_generate_tokenomics_text_success(mock_llm_client, nlg_engine):
    mock_llm_client.generate_text.return_value = {
        "choices": [{"message": {"content": "This is a generated tokenomics summary."}}]
    }
    raw_data = {"supply": "1B", "distribution": "fair"}
    
    result = await nlg_engine.generate_tokenomics_text(raw_data)
    
    expected_output = json.dumps({
        "section_id": "tokenomics",
        "text": "This is a generated tokenomics summary."
    })
    assert result == expected_output
    mock_llm_client.generate_text.assert_called_once()
    
    # Validate prompt correctness
    expected_template = get_template("tokenomics")
    expected_prompt = fill_template(
        expected_template,
        data=json.dumps(raw_data, indent=2)
    )
    mock_llm_client.generate_text.assert_called_with(expected_prompt)

@pytest.mark.asyncio
async def test_generate_tokenomics_text_missing_data(nlg_engine):
    result = await nlg_engine.generate_tokenomics_text({})
    expected_output = json.dumps({
        "section_id": "tokenomics",
        "text": "Tokenomics data is not available at this time. Please check back later for updates."
    })
    assert result == expected_output

@pytest.mark.asyncio
async def test_generate_tokenomics_text_empty_llm_response(mock_llm_client, nlg_engine):
    mock_llm_client.generate_text.return_value = {
        "choices": [{"message": {"content": ""}}]
    }
    raw_data = {"supply": "1B"}
    
    result = await nlg_engine.generate_tokenomics_text(raw_data)
    expected_output = json.dumps({
        "section_id": "tokenomics",
        "text": "Failed to generate tokenomics summary due to an internal error. Please try again later."
    })
    assert result == expected_output

@pytest.mark.asyncio
async def test_generate_tokenomics_text_llm_exception(mock_llm_client, nlg_engine):
    mock_llm_client.generate_text.side_effect = Exception("LLM connection error")
    raw_data = {"supply": "1B"}
    
    result = await nlg_engine.generate_tokenomics_text(raw_data)
    expected_output = json.dumps({
        "section_id": "tokenomics",
        "text": "Failed to generate tokenomics summary due to an internal error. Please try again later."
    })
    assert result == expected_output

@pytest.mark.asyncio
async def test_generate_onchain_text_success(mock_llm_client, nlg_engine):
    mock_llm_client.generate_text.return_value = {
        "choices": [{"message": {"content": "This is a generated on-chain metrics summary."}}]
    }
    raw_data = {"active_addresses": 1000, "holders": 500, "transaction_flows": "high", "liquidity": "good"}
    
    result = await nlg_engine.generate_onchain_text(raw_data)
    
    expected_output = json.dumps({
        "section_id": "onchain_metrics",
        "text": "This is a generated on-chain metrics summary."
    })
    assert result == expected_output
    mock_llm_client.generate_text.assert_called_once()
    
    # Validate prompt correctness
    expected_template = get_template("onchain_metrics")
    expected_prompt = fill_template(
        expected_template,
        data=json.dumps({
            "active_addresses": 1000,
            "holders": 500,
            "transaction_flows": "high",
            "liquidity": "good",
        }, indent=2)
    )
    mock_llm_client.generate_text.assert_called_with(expected_prompt)

@pytest.mark.asyncio
async def test_generate_onchain_text_missing_data(nlg_engine):
    result = await nlg_engine.generate_onchain_text({})
    expected_output = json.dumps({
        "section_id": "onchain_metrics",
        "text": "On-chain metrics data is not available at this time. Please check back later for updates."
    })
    assert result == expected_output

@pytest.mark.asyncio
async def test_generate_onchain_text_failed_status(nlg_engine):
    result = await nlg_engine.generate_onchain_text({"status": "failed"})
    expected_output = json.dumps({
        "section_id": "onchain_metrics",
        "text": "On-chain metrics data is not available at this time. Please check back later for updates."
    })
    assert result == expected_output

@pytest.mark.asyncio
async def test_generate_onchain_text_empty_llm_response(mock_llm_client, nlg_engine):
    mock_llm_client.generate_text.return_value = {
        "choices": [{"message": {"content": ""}}]
    }
    raw_data = {"active_addresses": 1000}
    
    result = await nlg_engine.generate_onchain_text(raw_data)
    expected_output = json.dumps({
        "section_id": "onchain_metrics",
        "text": "Failed to generate on-chain metrics summary due to an internal error. Please try again later."
    })
    assert result == expected_output

@pytest.mark.asyncio
async def test_generate_onchain_text_llm_exception(mock_llm_client, nlg_engine):
    mock_llm_client.generate_text.side_effect = Exception("LLM connection error")
    raw_data = {"active_addresses": 1000}
    
    result = await nlg_engine.generate_onchain_text(raw_data)
    expected_output = json.dumps({
        "section_id": "onchain_metrics",
        "text": "Failed to generate on-chain metrics summary due to an internal error. Please try again later."
    })
    assert result == expected_output

@pytest.mark.asyncio
async def test_generate_sentiment_text_success(mock_llm_client, nlg_engine):
    mock_llm_client.generate_text.return_value = {
        "choices": [{"message": {"content": "This is a generated social sentiment summary."}}]
    }
    raw_data = {"sentiment_score": 0.8, "trends": "positive"}
    
    result = await nlg_engine.generate_sentiment_text(raw_data)
    
    expected_output = json.dumps({
        "section_id": "social_sentiment",
        "text": "This is a generated social sentiment summary."
    })
    assert result == expected_output
    mock_llm_client.generate_text.assert_called_once()
    
    # Validate prompt correctness
    expected_template = get_template("social_sentiment")
    expected_prompt = fill_template(
        expected_template,
        data=json.dumps(raw_data, indent=2)
    )
    mock_llm_client.generate_text.assert_called_with(expected_prompt)

@pytest.mark.asyncio
async def test_generate_sentiment_text_missing_data(nlg_engine):
    result = await nlg_engine.generate_sentiment_text({})
    expected_output = json.dumps({
        "section_id": "social_sentiment",
        "text": "Social sentiment data is not available at this time. Please check back later for updates."
    })
    assert result == expected_output

@pytest.mark.asyncio
async def test_generate_sentiment_text_empty_llm_response(mock_llm_client, nlg_engine):
    mock_llm_client.generate_text.return_value = {
        "choices": [{"message": {"content": ""}}]
    }
    raw_data = {"sentiment_score": 0.8}
    
    result = await nlg_engine.generate_sentiment_text(raw_data)
    expected_output = json.dumps({
        "section_id": "social_sentiment",
        "text": "Failed to generate social sentiment summary due to an internal error. Please try again later."
    })
    assert result == expected_output

@pytest.mark.asyncio
async def test_generate_sentiment_text_llm_exception(mock_llm_client, nlg_engine):
    mock_llm_client.generate_text.side_effect = Exception("LLM connection error")
    raw_data = {"sentiment_score": 0.8}
    
    result = await nlg_engine.generate_sentiment_text(raw_data)
    expected_output = json.dumps({
        "section_id": "social_sentiment",
        "text": "Failed to generate social sentiment summary due to an internal error. Please try again later."
    })
    assert result == expected_output
